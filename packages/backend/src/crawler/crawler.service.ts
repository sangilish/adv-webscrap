import { Injectable, Logger, BadRequestException } from '@nestjs/common';
import { PrismaService } from '../prisma/prisma.service';
import { chromium, Browser, BrowserContext, Page } from 'playwright';
import * as fs from 'fs';
import * as path from 'path';

export interface CrawlResult {
  id: string;
  url: string;
  title: string;
  pageType: string;
  links: string[];
  images: string[];
  headings: { level: string; text: string }[];
  forms: number;
  buttons: string[];
  textContent: string;
  screenshotPath: string;
  htmlPath: string;
  timestamp: string;
  metadata: {
    wordCount: number;
    imageCount: number;
    linkCount: number;
  };
}

export interface NetworkNode {
  id: string;
  label: string;
  color: string;
  type: string;
  url: string;
  title: string;
  screenshot: string;
}

export interface NetworkEdge {
  from: string;
  to: string;
}

export interface NetworkData {
  nodes: NetworkNode[];
  edges: NetworkEdge[];
}

export interface AnalysisResult {
  results: CrawlResult[];
  networkData: NetworkData;
  totalPages: number;
  isPreview: boolean;
  previewLimit: number;
  message: string;
}

@Injectable()
export class CrawlerService {
  private readonly logger = new Logger(CrawlerService.name);

  constructor(private prisma: PrismaService) {}

  async startCrawling(
    userId: number,
    targetUrl: string,
    maxPages: number = 5,
  ): Promise<string> {
    // ì‚¬ìš©ì í”Œëœ í™•ì¸
    const user = await this.prisma.user.findUnique({
      where: { id: userId },
      include: { analyses: true }
    });

    if (!user) {
      throw new BadRequestException('User not found');
    }

    // Free í”Œëœ ì œí•œ í™•ì¸ (ì›” 10ê°œ)
    const currentMonth = new Date();
    currentMonth.setDate(1);
    currentMonth.setHours(0, 0, 0, 0);
    
    const monthlyAnalyses = user.analyses.filter(
      analysis => new Date(analysis.createdAt) >= currentMonth
    );

    if (user.plan === 'FREE' && monthlyAnalyses.length >= 10) {
      throw new BadRequestException('Monthly crawling limit reached. Please upgrade to Pro.');
    }

    // ë¶„ì„ ë ˆì½”ë“œ ìƒì„±
    const analysis = await this.prisma.analysis.create({
      data: {
        userId,
        url: targetUrl,
        status: 'running',
      },
    });

    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    this.performCrawling(analysis.id, targetUrl, maxPages, user.plan).catch((error) => {
      this.logger.error(`Crawling failed for analysis ${analysis.id}:`, error);
      this.prisma.analysis.update({
        where: { id: analysis.id },
        data: { status: 'failed' },
      });
    });

    return analysis.id;
  }

  private async performCrawling(
    analysisId: string,
    startUrl: string,
    maxPages: number,
    userPlan: string,
  ): Promise<void> {
    const results: CrawlResult[] = [];
    const visitedUrls = new Set<string>();
    const baseUrl = new URL(startUrl).origin;
    
    // í”Œëœë³„ í˜ì´ì§€ ì œí•œ (FREEë„ ì¶©ë¶„íˆ íƒìƒ‰ ê°€ëŠ¥í•˜ë„ë¡ í™•ëŒ€)
    const planLimits = {
      FREE: 100,
      PRO: 500,
      ENTERPRISE: 1000
    };
    
    const actualMaxPages = Math.min(maxPages, planLimits[userPlan] || 5);
    
    // ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    const outputDir = path.join(process.cwd(), 'uploads', analysisId);
    const screenshotsDir = path.join(outputDir, 'screenshots');
    const htmlDir = path.join(outputDir, 'html');
    
    await fs.promises.mkdir(screenshotsDir, { recursive: true });
    await fs.promises.mkdir(htmlDir, { recursive: true });

    const browser = await chromium.launch({ 
      headless: true
    });
    const context = await browser.newContext({
      userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
      viewport: { width: 1400, height: 900 }
    });

    try {
      const urlsToVisit = [startUrl];
      
      while (urlsToVisit.length > 0 && results.length < actualMaxPages) {
        const currentUrl = urlsToVisit.shift();
        
        if (!currentUrl || visitedUrls.has(currentUrl)) {
          continue;
        }
        
        visitedUrls.add(currentUrl);
        this.logger.log(`Crawling: ${currentUrl} (${results.length + 1}/${actualMaxPages})`);

        const page = await context.newPage();
        // í˜ì´ì§€ ì½˜ì†” ì—ëŸ¬ ë¡œê¹…
        page.on('pageerror', (err) => this.logger.error('Page error:', err));
        
        try {
          // ChatGPT ë¶„ì„ ë°˜ì˜: networkidle íƒ€ì„ì•„ì›ƒ ë¬¸ì œ í•´ê²°
          console.log(`ğŸ” Loading page: ${currentUrl}`);
          const response = await page.goto(currentUrl, { 
            waitUntil: 'domcontentloaded', 
            timeout: 30000 
          });

          // ì‘ë‹µ ìƒíƒœ í™•ì¸
          if (!response || !response.ok()) {
            console.log(`âŒ Page load failed with status: ${response?.status()}`);
            continue;
          }

          // Curated.media WebSocket/SSE ì§€ì† ëŒ€ì‘: networkidle ëŒ€ì‹  DOM ë¡œë“œ í›„ ëŒ€ê¸°
          await page.waitForTimeout(3000);
          console.log(`âœ… Page loaded and ready: ${currentUrl}`);

          // ë””ë²„ê¹…: í˜ì´ì§€ ë‚´ìš© í™•ì¸
          const pageTitle = await page.title();
          const pageContentLength = (await page.content()).length;
          console.log(`ğŸ“Š Page Debug Info:`);
          console.log(`  - Title: "${pageTitle}"`);
          console.log(`  - Content Length: ${pageContentLength} chars`);
          console.log(`  - Response Status: ${response?.status()}`);

          // NEW: ê³ ì • ë·°í¬íŠ¸ ì„¤ì • (ì „ì²´ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ì•ˆì •í™”)
          await page.setViewportSize({ width: 1280, height: 800 });

          // SPA ë„¤ë¹„ê²Œì´ì…˜ íƒì§€ (ì²« ë²ˆì§¸ í˜ì´ì§€ì—ì„œë§Œ)
          if (results.length === 0) {
            console.log('ğŸ” Detecting SPA navigation patterns...');
            const spaRoutes = await this.detectSPANavigation(page, baseUrl);
            console.log(`ğŸ¯ Found ${spaRoutes.length} potential SPA routes`);
            
            // SPA ë¼ìš°íŠ¸ë¥¼ í¬ë¡¤ë§ íì— ì¶”ê°€
            for (const route of spaRoutes) {
              if (!visitedUrls.has(route) && !urlsToVisit.includes(route)) {
                urlsToVisit.push(route);
                console.log(`â• Added SPA route to queue: ${route}`);
              }
            }
          }

          // ë°ì´í„° ì¶”ì¶œ
          const result = await this.crawlSinglePage(page, currentUrl, outputDir);
          results.push(result);

          // ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
          await this.prisma.analysis.update({
            where: { id: analysisId },
            data: { 
              pageCount: results.length,
              progress: Math.round((results.length / actualMaxPages) * 100)
            },
          });

          // ê°™ì€ ë„ë©”ì¸ì˜ ìƒˆë¡œìš´ ë§í¬ ì°¾ê¸°
          if (results.length < actualMaxPages) {
            const links = await this.extractLinks(page);
            
            for (const link of links) {
              try {
                const linkUrl = new URL(link);
                if (linkUrl.origin === baseUrl && !visitedUrls.has(link) && !urlsToVisit.includes(link)) {
                  urlsToVisit.push(link);
                }
              } catch (e) {
                // ì˜ëª»ëœ URL ë¬´ì‹œ
              }
            }
          }
        } catch (error) {
          this.logger.error(`Error crawling ${currentUrl}:`, error);
        } finally {
          await page.close();
        }
      }
      
      // ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„±
      const networkData = this.generateNetworkData(results, {});
      
      // ì‹œê°í™” HTML ìƒì„±
      const visualizationHtml = this.generateVisualizationHtml(networkData, results);
      const htmlPath = path.join(outputDir, 'visualization.html');
      await fs.promises.writeFile(htmlPath, visualizationHtml);

      // ë¶„ì„ ì™„ë£Œ ì—…ë°ì´íŠ¸
      await this.prisma.analysis.update({
        where: { id: analysisId },
        data: {
          status: 'completed',
          pageCount: results.length,
          progress: 100,
        },
      });

      this.logger.log(`Crawling completed for analysis ${analysisId}. ${results.length} pages crawled.`);
    } catch (error) {
      this.logger.error(`Crawling failed for analysis ${analysisId}:`, error);
      await this.prisma.analysis.update({
        where: { id: analysisId },
        data: { status: 'failed' },
      });
    } finally {
      await browser.close();
    }
  }

  private async removeCookiePopups(page: any): Promise<void> {
    // ë‹¤ì–‘í•œ ì¿ í‚¤ íŒì—… selectors
    const cookieSelectors = [
      // Didomi ê´€ë ¨
      '[id="didomi-notice"]',
      '[class*="didomi"]',
      '#didomi-popup',
      '#didomi-banner',
      
      // ì¼ë°˜ì ì¸ ì¿ í‚¤ ê´€ë ¨
      '[id*="cookie"]',
      '[class*="cookie"]',
      '[data-testid*="cookie"]',
      '[aria-label*="cookie"]',
      
      // Consent ê´€ë ¨
      '[id*="consent"]',
      '[class*="consent"]',
      '[data-testid*="consent"]',
      
      // GDPR ê´€ë ¨
      '[id*="gdpr"]',
      '[class*="gdpr"]',
      
      // Banner/Modal/Popup ê´€ë ¨
      '[class*="banner"]',
      '[class*="popup"]',
      '[class*="modal"]',
      '[class*="overlay"]',
      '[role="dialog"]',
      '[role="banner"]',
      
      // íŠ¹ì • í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì†Œë“¤
      'div:has-text("cookie")',
      'div:has-text("consent")',
      'div:has-text("privacy")',
      'div:has-text("accept")'
    ];

    // ë¨¼ì € ëª¨ë“  ì¿ í‚¤ ê´€ë ¨ ìš”ì†Œ ì œê±°
    for (const selector of cookieSelectors) {
      try {
        await page.waitForTimeout(500); // íŒì—… ë¡œë“œ ëŒ€ê¸°
        const elements = await page.$$(selector);
        for (const element of elements) {
          try {
            const isVisible = await element.isVisible();
            const boundingBox = await element.boundingBox();
            if (isVisible || boundingBox) {
              await element.evaluate((el) => {
                el.style.display = 'none !important';
                el.style.visibility = 'hidden !important';
                el.style.opacity = '0 !important';
                el.remove();
              });
            }
          } catch (e) {
            // ê°œë³„ ìš”ì†Œ ì²˜ë¦¬ ì‹¤íŒ¨ ë¬´ì‹œ
          }
        }
      } catch (error) {
        // selector ì²˜ë¦¬ ì‹¤íŒ¨ ë¬´ì‹œ
      }
    }

    // Accept/ë™ì˜ ë²„íŠ¼ë“¤ í´ë¦­ ì‹œë„
    const acceptSelectors = [
      '#didomi-notice-agree-button',
      '#didomi-notice-agree-to-all',
      '.didomi-continue-without-agreeing',
      'button:has-text("Accept")',
      'button:has-text("Accept All")',
      'button:has-text("ë™ì˜")',
      'button:has-text("ëª¨ë‘ ë™ì˜")',
      'button:has-text("OK")',
      'button:has-text("Got it")',
      'button:has-text("I understand")',
      '[class*="accept"]',
      '[id*="accept"]',
      '[data-testid*="accept"]'
    ];

    for (const selector of acceptSelectors) {
      try {
        const button = await page.$(selector);
        if (button) {
          const isVisible = await button.isVisible();
          if (isVisible) {
            await button.click({ force: true });
            await page.waitForTimeout(1000);
            console.log(`Clicked cookie accept button: ${selector}`);
            break;
          }
        }
      } catch (error) {
        // ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ ë¬´ì‹œ
      }
    }

    // CSSë¡œ ê°•ì œ ìˆ¨ê¹€ ì²˜ë¦¬
    await page.addStyleTag({
      content: `
        [id*="didomi"],
        [class*="didomi"],
        [id*="cookie"],
        [class*="cookie"],
        [id*="consent"],
        [class*="consent"],
        [id*="gdpr"],
        [class*="gdpr"] {
          display: none !important;
          visibility: hidden !important;
          opacity: 0 !important;
          height: 0 !important;
          width: 0 !important;
          z-index: -9999 !important;
        }
      `
    });

    // ì¶”ê°€ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ íŒì—… ì™„ì „ ì œê±° í™•ì¸
    await page.waitForTimeout(2000);
  }

  private async crawlSinglePage(page: Page, url: string, outputDir: string): Promise<CrawlResult> {
    const timestamp = new Date().toISOString();
    const pageId = `page_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    console.log(`ğŸš€ í˜ì´ì§€ ë¡œë”© ì‹œì‘: ${url}`);
    
    // Enable console logging from the page
    page.on('console', msg => {
      console.log(`[PAGE ${msg.type().toUpperCase()}] ${msg.text()}`);
    });
    
    // Navigate to the page first!
    await page.goto(url, { 
      waitUntil: 'domcontentloaded', 
      timeout: 30000 
    });

    console.log(`ğŸ“„ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ, SPA ë Œë”ë§ ëŒ€ê¸° ì¤‘...`);

    // Wait for the page to be fully interactive
    await page.waitForLoadState('networkidle').catch(() => {
      console.log('âš ï¸ ë„¤íŠ¸ì›Œí¬ idle ëŒ€ê¸° ì‹¤íŒ¨, DOM ë¡œë“œë¡œ ëŒ€ì²´');
      return page.waitForLoadState('domcontentloaded');
    });
    
    // Give SPAs extra time to render - curated.media needs more time
    console.log(`â³ SPA ë Œë”ë§ì„ ìœ„í•´ 5ì´ˆ ëŒ€ê¸°...`);
    await page.waitForTimeout(5000);

    // Remove cookie popups only for full analysis (í”„ë¦¬ë·°ì—ì„œëŠ” ìƒëµ)
    const isPreviewRun = path.basename(outputDir).startsWith('preview_');
    if (!isPreviewRun) {
      console.log(`ğŸª ì¿ í‚¤ íŒì—… ë° ì˜¤ë²„ë ˆì´ ì œê±° ì¤‘...`);
      await this.removeCookiePopups(page);
      await page.waitForTimeout(1000);
    }

    // Try to trigger any lazy-loaded content
    console.log(`ğŸ“œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì§€ì—° ë¡œë”© ì»¨í…ì¸  íŠ¸ë¦¬ê±°...`);
    await page.evaluate(() => {
      // Scroll to trigger lazy loading
      window.scrollTo(0, document.body.scrollHeight);
      window.scrollTo(0, 0);
    });
    await page.waitForTimeout(2000);

    // Screenshot
    const screenshotFilename = `${pageId}.png`;
    const screenshotsDir = path.join(outputDir, 'screenshots');
    if (!fs.existsSync(screenshotsDir)) {
      fs.mkdirSync(screenshotsDir, { recursive: true });
    }
    const screenshotPath = path.join(screenshotsDir, screenshotFilename);
    await page.screenshot({
      path: screenshotPath,
      fullPage: true,
      type: 'png'
    });

    // HTML content
    const htmlContent = await page.content();
    const htmlFilename = `${pageId}.html`;
    const htmlDir = path.join(outputDir, 'html');
    if (!fs.existsSync(htmlDir)) {
      fs.mkdirSync(htmlDir, { recursive: true });
    }
    const htmlPath = path.join(htmlDir, htmlFilename);
    await fs.promises.writeFile(htmlPath, htmlContent);

    // Extract all possible navigation targets
    console.log(`ğŸ” ë§í¬ ì¶”ì¶œ ì‹œì‘: ${url}`);
    
    const navigationData = await page.evaluate(() => {
      const currentOrigin = location.origin;

      // 1) ëª¨ë“  a[href] ì ˆëŒ€ URL ëª¨ìœ¼ê¸°
      const rawLinks = Array.from(document.querySelectorAll('a[href]'))
        .map(a => (a as HTMLAnchorElement).href.trim())
        .filter(h => h &&
                    !h.startsWith('javascript:') &&
                    !h.startsWith('mailto:') &&
                    !h.startsWith('tel:'));

      // 2) ê°™ì€ Origin ë§Œ í•„í„° & ì¤‘ë³µ ì œê±°
      const links = Array.from(new Set(
        rawLinks.filter(h => {
          try { return new URL(h).origin === currentOrigin; } catch { return false; }
        })
      ));

      // ê¸°ë³¸ í˜ì´ì§€ ë©”íƒ€ ë°ì´í„°
      const images = Array.from(document.querySelectorAll('img[src]'))
        .map(img => (img as HTMLImageElement).src)
        .filter(src => src && !src.startsWith('data:'));

      const headings = Array.from(document.querySelectorAll('h1,h2,h3,h4,h5,h6'))
        .map(h => ({ level: h.tagName.toLowerCase(), text: h.textContent?.trim() || '' }))
        .filter(h => h.text);

      const forms = document.querySelectorAll('form').length;
      const buttonTexts = Array.from(document.querySelectorAll('button,input[type="button"],input[type="submit"]'))
        .map(btn => btn.textContent?.trim() || btn.getAttribute('value') || '')
        .filter(t => t);

      return {
        title: document.title || '',
        links,
        allLinks: rawLinks,
        images,
        headings,
        forms,
        textContent: document.body?.innerText || '',
        buttons: buttonTexts,
        debugInfo: [] as any[]
      };
    });

    // Log debug information
    console.log(`\nğŸ” Link Extraction Debug for ${url}:`);
    console.log(`Found ${navigationData.links.length} valid links`);
    console.log(`Total links discovered: ${navigationData.allLinks ? navigationData.allLinks.length : 0}`);
    
    // Temporary: Add debug info to textContent for inspection
    let debugText = `\n\n=== DEBUG INFO ===\n`;
    debugText += `Total links found: ${navigationData.links.length}\n`;
    debugText += `All links discovered: ${navigationData.allLinks ? navigationData.allLinks.length : 0}\n`;
    debugText += `Debug entries: ${navigationData.debugInfo ? navigationData.debugInfo.length : 0}\n`;
    
    if (navigationData.allLinks) {
      debugText += `\nAll discovered links:\n`;
      navigationData.allLinks.slice(0, 10).forEach(link => debugText += `  - ${link}\n`);
      if (navigationData.allLinks.length > 10) {
        debugText += `  ... and ${navigationData.allLinks.length - 10} more\n`;
      }
    }
    
    if (navigationData.debugInfo) {
      const accepted = navigationData.debugInfo.filter(d => d.accepted);
      const rejected = navigationData.debugInfo.filter(d => !d.accepted);
      
      debugText += `\nAccepted: ${accepted.length} links\n`;
      accepted.slice(0, 5).forEach(d => debugText += `  - ${d.url} (${d.source})\n`);
      
      debugText += `\nRejected: ${rejected.length} links\n`;
      const rejectionReasons = rejected.reduce((acc, d) => {
        acc[d.rejected] = (acc[d.rejected] || 0) + 1;
        return acc;
      }, {} as Record<string, number>);
      
      Object.entries(rejectionReasons).forEach(([reason, count]) => {
        debugText += `  - ${reason}: ${count} links\n`;
      });
      
      // Add first few rejected links for inspection
      debugText += `\nFirst few rejected:\n`;
      rejected.slice(0, 5).forEach(d => debugText += `  - ${d.url} (${d.rejected}, ${d.source})\n`);
    }
    
    // Append debug info to textContent temporarily
    navigationData.textContent += debugText;
    
    if (navigationData.debugInfo) {
      const accepted = navigationData.debugInfo.filter(d => d.accepted);
      const rejected = navigationData.debugInfo.filter(d => !d.accepted);
      
      console.log(`âœ… Accepted: ${accepted.length} links`);
      accepted.slice(0, 5).forEach(d => console.log(`  - ${d.url} (${d.source})`));
      
      console.log(`âŒ Rejected: ${rejected.length} links`);
      const rejectionReasons = rejected.reduce((acc, d) => {
        acc[d.rejected] = (acc[d.rejected] || 0) + 1;
        return acc;
      }, {} as Record<string, number>);
      
      Object.entries(rejectionReasons).forEach(([reason, count]) => {
        console.log(`  - ${reason}: ${count} links`);
      });
    }

         // Page type classification
     let pageType = 'ì¼ë°˜í˜ì´ì§€';
     const pathname = new URL(url).pathname.toLowerCase();
     const titleLower = navigationData.title.toLowerCase();
     
     if (pathname === '/' || pathname === '/home' || titleLower.includes('home')) {
       pageType = 'í™ˆí˜ì´ì§€';
     } else if (pathname.includes('about') || titleLower.includes('about')) {
       pageType = 'ì†Œê°œí˜ì´ì§€';
     } else if (pathname.includes('contact') || titleLower.includes('contact')) {
       pageType = 'ì—°ë½ì²˜';
     } else if (pathname.includes('product') || titleLower.includes('product')) {
       pageType = 'ì œí’ˆí˜ì´ì§€';
     } else if (pathname.includes('service') || titleLower.includes('service')) {
       pageType = 'ì„œë¹„ìŠ¤í˜ì´ì§€';
     } else if (pathname.includes('dashboard') || titleLower.includes('dashboard')) {
       pageType = 'ëŒ€ì‹œë³´ë“œ';
     }

    const isPreview = path.basename(outputDir).startsWith('preview_');
    const screenshotUrl = isPreview
      ? `/temp/${path.basename(outputDir)}/screenshots/${screenshotFilename}`
      : `/uploads/${path.basename(outputDir)}/screenshots/${screenshotFilename}`;
    const htmlUrl = isPreview
      ? `/temp/${path.basename(outputDir)}/html/${htmlFilename}`
      : `/uploads/${path.basename(outputDir)}/html/${htmlFilename}`;

    // Add debugging info
    console.log(`ğŸ“Š Navigation data:`, {
      title: navigationData.title,
      linksFound: navigationData.links.length,
      debugEntries: navigationData.debugInfo?.length || 0
    });
    
    const result: CrawlResult = {
      id: pageId,
      url,
      title: `${navigationData.title} [LINKS: ${navigationData.links.length}]`,
      pageType,
      links: navigationData.links,
      images: navigationData.images,
      headings: navigationData.headings,
      forms: navigationData.forms,
      buttons: navigationData.buttons,
      textContent: navigationData.textContent + `\n\n=== DEBUG ===\nLinks found: ${navigationData.links.length}\nDebug entries: ${navigationData.debugInfo?.length || 0}`,
      screenshotPath: screenshotUrl,
      htmlPath: htmlUrl,
      timestamp,
      metadata: {
        wordCount: navigationData.textContent.split(/\s+/).length,
        imageCount: navigationData.images.length,
        linkCount: navigationData.links.length
      }
    };

    return result;
  }

  private async extractLinks(page: Page): Promise<string[]> {
    const base = new URL(page.url()).origin;

    // 1) ë©”ë‰´(ì‚¬ì´ë“œë°”)ê°€ ë Œë”ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸°
    await page.waitForSelector('nav, .sidebar, [role="menu"]', { timeout: 5000 }).catch(() => {});

    // 2) ëª¨ë“  <a> íƒœê·¸ì˜ ì ˆëŒ€ URL
    const anchors = await page.$$eval('a[href]', els =>
      els
        .map(a => (a as HTMLAnchorElement).href)
        .filter(href => href.startsWith(window.location.origin) &&
                        !/(javascript:|mailto:|tel:)/.test(href) &&
                        !/\.(css|js|png|jpg|jpeg|gif|svg|ico|pdf|zip|mp4|mp3)([?#]|$)/i.test(href))
    );

    // 3) onclick ì†ì„±ìœ¼ë¡œ ë„¤ë¹„ê²Œì´ì…˜ í•˜ëŠ” ë²„íŠ¼
    const btns = await page.$$eval('button[onclick]', els =>
      els
        .map(b => {
          const m = b.getAttribute('onclick')?.match(/location\.href\s*=\s*['"]([^'"]+)['"]/);
          return m ? new URL(m[1], window.location.href).href : null;
        })
        .filter((u): u is string => !!u && u.startsWith(window.location.origin))
    );

    // 4) data-href / data-url ì†ì„±
    const dataLinks = await page.$$eval('[data-href],[data-url]', els =>
      els
        .map(el => el.getAttribute('data-href') || el.getAttribute('data-url'))
        .map(url => url ? new URL(url, window.location.href).href : null)
        .filter((u): u is string => !!u && u.startsWith(window.location.origin))
    );

    // 5) ì‹¤ì œ í´ë¦­ìœ¼ë¡œ ë¼ìš°íŒ…ë˜ëŠ” ë§í¬ (SPA)
    const clickItems = await page.$$('nav button, nav li, [role="menuitem"]');
    const clickLinks: string[] = [];
    for (const item of clickItems) {
      try {
        const before = page.url();
        await item.click();
        await page.waitForLoadState('networkidle');
        const after = page.url();
        if (after !== before && after.startsWith(base)) {
          clickLinks.push(after);
        }
        // ë’¤ë¡œ ëŒì•„ê°€ê¸°
        await page.goBack({ waitUntil: 'networkidle' });
      } catch {
        // í´ë¦­ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
      } finally {
        await page.waitForTimeout(500);
      }
    }

    // 6) ëª¨ë‘ í•©ì³ì„œ ì¤‘ë³µ ì œê±°
    const all = [...anchors, ...btns, ...dataLinks, ...clickLinks];
    return Array.from(new Set(all));
  }

  private generateNetworkData(results: CrawlResult[], sitemap: Record<string, string[]>): NetworkData {
    const nodes: NetworkNode[] = [];
    const edges: NetworkEdge[] = [];
    const processedUrls = new Set<string>();
    
    results.forEach((result, index) => {
      if (!processedUrls.has(result.url)) {
        processedUrls.add(result.url);
        
        // Determine node color based on page type
        let color = '#6366f1'; // Default purple
        switch (result.pageType) {
          case 'í™ˆí˜ì´ì§€': color = '#ef4444'; break;
          case 'ì†Œê°œí˜ì´ì§€': color = '#10b981'; break;
          case 'ì—°ë½ì²˜': color = '#f59e0b'; break;
          case 'ì œí’ˆí˜ì´ì§€': color = '#8b5cf6'; break;
          case 'ì„œë¹„ìŠ¤í˜ì´ì§€': color = '#06b6d4'; break;
          case 'ëŒ€ì‹œë³´ë“œ': color = '#ec4899'; break;
        }
        
        nodes.push({
          id: result.id,
          label: result.title.substring(0, 20) + (result.title.length > 20 ? '...' : ''),
          color,
          type: result.pageType,
          url: result.url,
          title: result.title,
          screenshot: result.screenshotPath
        });
      }
    });
    
    // Generate edges from sitemap
    Object.entries(sitemap).forEach(([parentUrl, childUrls]) => {
      const parentResult = results.find(r => r.url === parentUrl);
      if (parentResult) {
        childUrls.forEach(childUrl => {
          const childResult = results.find(r => r.url === childUrl);
          if (childResult) {
            edges.push({
              from: parentResult.id,
              to: childResult.id
            });
          }
        });
      }
    });
    
    return { nodes, edges };
  }

  private generateVisualizationHtml(networkData: NetworkData, results: CrawlResult[]): string {
    // ê¸°ì¡´ ì‹œê°í™” HTML ìƒì„± ì½”ë“œ ìœ ì§€
    return `
<!DOCTYPE html>
<html>
<head>
    <title>Website Structure Visualization</title>
    <script src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; }
        #network { height: 600px; border: 1px solid #ccc; }
        .info-panel { margin-top: 20px; padding: 15px; background: #f5f5f5; border-radius: 8px; }
        .page-info { margin: 10px 0; padding: 10px; background: white; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>Website Structure Analysis</h1>
    <div id="network"></div>
    <div class="info-panel">
        <h3>Analysis Summary</h3>
        <p><strong>Total Pages:</strong> ${results.length}</p>
        <p><strong>Total Links:</strong> ${results.reduce((sum, r) => sum + r.links.length, 0)}</p>
        <p><strong>Total Images:</strong> ${results.reduce((sum, r) => sum + r.images.length, 0)}</p>
    </div>
    
    <script>
        const nodes = new vis.DataSet(${JSON.stringify(networkData.nodes)});
        const edges = new vis.DataSet(${JSON.stringify(networkData.edges)});
        const container = document.getElementById('network');
        const data = { nodes: nodes, edges: edges };
        const options = {
            nodes: {
                shape: 'dot',
                size: 20,
                font: { size: 12 }
            },
            edges: {
                arrows: 'to'
            },
            physics: {
                enabled: true,
                stabilization: { iterations: 100 }
            }
        };
        const network = new vis.Network(container, data, options);
    </script>
</body>
</html>`;
  }

  // ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
  private sanitizeFilename(url: string): string {
    return url.replace(/[^0-9a-zA-Z]+/g, '_').slice(0, 200);
  }

  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private randomDelay(min: number = 1000, max: number = 3000): number {
    return Math.random() * (max - min) + min;
  }

  private isSameDomain(url: string, baseUrl: string): boolean {
    try {
      const urlObj = new URL(url);
      const baseUrlObj = new URL(baseUrl);
      return urlObj.hostname === baseUrlObj.hostname;
    } catch {
      return false;
    }
  }

  // ì‚¬ìš©ìë³„ ë¶„ì„ ì¡°íšŒ (Free ìœ ì €ëŠ” 5ê°œë§Œ)
  async getUserAnalyses(userId: number, limit?: number) {
    const user = await this.prisma.user.findUnique({
      where: { id: userId }
    });

    const actualLimit = user?.plan === 'FREE' ? 5 : (limit || 50);

    return this.prisma.analysis.findMany({
      where: { userId },
      orderBy: { createdAt: 'desc' },
      take: actualLimit,
      select: {
        id: true,
        url: true,
        title: true,
        status: true,
        pageCount: true,
        progress: true,
        createdAt: true,
        updatedAt: true
      }
    });
  }

  // ë¶„ì„ ìƒì„¸ ì¡°íšŒ
  async getAnalysis(analysisId: string, userId: number) {
    const analysis = await this.prisma.analysis.findFirst({
      where: { 
        id: analysisId, 
        userId 
      }
    });

    if (!analysis) {
      throw new BadRequestException('Analysis not found');
    }

    return {
      ...analysis,
      resultData: analysis.resultData ? JSON.parse(analysis.resultData) : null
    };
  }

  // ë¬´ë£Œ ë¯¸ë¦¬ë³´ê¸° (ë¡œê·¸ì¸ ì—†ì´ 5ê°œ í˜ì´ì§€ë§Œ)
  async getPreviewAnalysis(url: string): Promise<AnalysisResult> {
    this.logger.log(`Starting preview analysis for: ${url}`);
    
    try {
      // Validate URL
      new URL(url);
      
      // Perform DFS crawl with preview limits
      const { results, sitemap } = await this.performDFSCrawl(url, 2, 5); // depth=2, max=5 pages for preview
      
      // Generate network data
      const networkData = this.generateNetworkData(results, sitemap);
      
      return {
        results,
        networkData,
        totalPages: results.length,
        isPreview: true,
        previewLimit: 5,
        message: `ë¯¸ë¦¬ë³´ê¸°ë¡œ ${results.length}ê°œ í˜ì´ì§€ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë¶„ì„ì„ ì›í•˜ì‹œë©´ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.`
      };
      
    } catch (error) {
      this.logger.error(`Preview analysis failed for ${url}:`, error);
      throw error;
    }
  }

  private async performDFSCrawl(
    startUrl: string, 
    maxDepth: number = 3, 
    maxPages: number = 5
  ): Promise<{ results: CrawlResult[], sitemap: Record<string, string[]> }> {
    
    // Create output directory
    const timestamp = Date.now();
    const outputDir = path.join(process.cwd(), 'temp', `preview_${timestamp}`);
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }
    
    let browser: Browser | null = null;
    let context: BrowserContext | null = null;
    
    try {
      // Launch browser
      browser = await chromium.launch({ 
        headless: true,
        args: ['--no-sandbox', '--disable-dev-shm-usage']
      });
      
      context = await browser.newContext({
        userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36',
        viewport: { width: 1400, height: 900 }
      });
      
      const page = await context.newPage();
      
      const visited = new Set<string>();
      const sitemap: Record<string, string[]> = {};
      const results: CrawlResult[] = [];
      
      // DFS crawling function
      const dfs = async (url: string, depth: number): Promise<void> => {
        if (depth > maxDepth || visited.has(url) || results.length >= maxPages) {
          this.logger.log(`â­ï¸  Skipping ${url} - depth:${depth}/${maxDepth}, visited:${visited.has(url)}, results:${results.length}/${maxPages}`);
          return;
        }
        
        this.logger.log(`ğŸ” [DEPTH ${depth}] Starting crawl: ${url}`);
        visited.add(url);
        
        try {
          // Navigate to the page
          await page.goto(url, { waitUntil: 'domcontentloaded', timeout: 30000 });
          await page.waitForTimeout(3000);
          
          // SPA ë„¤ë¹„ê²Œì´ì…˜ íƒì§€ (ì²« ë²ˆì§¸ í˜ì´ì§€ì—ì„œë§Œ)
          if (depth === 0) {
            console.log('ğŸ” Detecting SPA navigation patterns...');
            const spaRoutes = await this.detectSPANavigation(page, new URL(startUrl).origin);
            console.log(`ğŸ¯ Found ${spaRoutes.length} potential SPA routes`);
            
            // SPA ë¼ìš°íŠ¸ë¥¼ ë°©ë¬¸ ëª©ë¡ì— ì¶”ê°€
            for (const route of spaRoutes) {
              if (!visited.has(route)) {
                visited.add(route);
                console.log(`â• Added SPA route: ${route}`);
              }
            }
          }
          
          const result = await this.crawlSinglePage(page, url, outputDir);
          results.push(result);
          
          this.logger.log(`âœ… [DEPTH ${depth}] Crawled successfully: ${url} (found ${result.links.length} links)`);
          
          // Filter same-domain links
          const childLinks = result.links.filter(link => this.isSameDomain(link, startUrl));
          sitemap[url] = childLinks;
          
          this.logger.log(`ğŸ”— [DEPTH ${depth}] Same-domain child links: ${childLinks.length}`);
          childLinks.forEach((link, index) => {
            this.logger.log(`   ${index + 1}. ${link}`);
          });
          
          // Recursively crawl child links
          for (const childUrl of childLinks) {
            if (results.length >= maxPages) {
              this.logger.log(`ğŸ›‘ Reached max pages limit (${maxPages})`);
              break;
            }
            
            if (!visited.has(childUrl)) {
              this.logger.log(`â³ [DEPTH ${depth}] Preparing to crawl child: ${childUrl}`);
              
              // Random delay between requests
              const delay = this.randomDelay(1000, 3000);
              this.logger.log(`â° Waiting ${Math.round(delay)}ms before next crawl...`);
              await this.sleep(delay);
              
              await dfs(childUrl, depth + 1);
            } else {
              this.logger.log(`â­ï¸  Already visited: ${childUrl}`);
            }
          }
          
        } catch (error) {
          this.logger.warn(`âŒ [DEPTH ${depth}] Failed to crawl ${url}: ${error.message}`);
          sitemap[url] = [];
        }
      };
      
      // Start DFS from root URL
      await dfs(startUrl, 0);
      
      // Save sitemap
      const sitemapPath = path.join(outputDir, 'sitemap.json');
      fs.writeFileSync(sitemapPath, JSON.stringify(sitemap, null, 2));
      
      this.logger.log(`Crawl finished - ${results.length} pages crawled`);
      
      return { results, sitemap };
      
    } finally {
      if (context) await context.close();
      if (browser) await browser.close();
    }
  }

  // íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ë¡œê·¸ì¸ í•„ìš”)
  async downloadFile(analysisId: string, userId: number, fileType: 'png' | 'html', pageId: string) {
    const analysis = await this.prisma.analysis.findFirst({
      where: { 
        id: analysisId, 
        userId,
        status: 'completed'
      }
    });

    if (!analysis) {
      throw new BadRequestException('Analysis not found or not completed');
    }

    const resultData = analysis.resultData ? JSON.parse(analysis.resultData) : null;
    if (!resultData?.results) {
      throw new BadRequestException('No results found');
    }

    const page = resultData.results.find(r => r.id === pageId);
    if (!page) {
      throw new BadRequestException('Page not found');
    }

    const filePath = fileType === 'png' ? page.screenshotPath : page.htmlPath;
    const fullPath = path.join(process.cwd(), filePath.replace('/uploads/', 'uploads/'));

    if (!fs.existsSync(fullPath)) {
      throw new BadRequestException('File not found');
    }

    return {
      filePath: fullPath,
      filename: path.basename(fullPath),
      contentType: fileType === 'png' ? 'image/png' : 'text/html'
    };
  }

  // Add SPA detection method
  private async detectSPANavigation(page: Page, baseUrl: string): Promise<string[]> {
    const discoveredUrls = new Set<string>();
    
    console.log('ğŸ” Detecting SPA navigation patterns...');
    
    // Method 1: Intercept navigation requests
    page.on('request', request => {
      const url = request.url();
      const resourceType = request.resourceType();
      
      // Look for API calls that might indicate routes
      if ((resourceType === 'xhr' || resourceType === 'fetch') && url.startsWith(baseUrl)) {
        console.log(`ğŸŒ Detected API call: ${url}`);
        
        // Extract potential routes from API paths
        try {
          const urlObj = new URL(url);
          const pathSegments = urlObj.pathname.split('/').filter(s => s);
          
          // Common patterns: /api/pages/*, /api/*/list, etc.
          if (pathSegments.includes('pages') || pathSegments.includes('routes')) {
            discoveredUrls.add(url);
          }
        } catch (e) {
          // Invalid URL
        }
      }
    });
    
    // Method 2: Monitor for client-side route changes
    await page.evaluate(() => {
      // Intercept History API
      const originalPushState = history.pushState;
      const originalReplaceState = history.replaceState;
      
      history.pushState = function(...args) {
        console.log('ğŸ”€ pushState:', args[2]);
        window.postMessage({ type: 'navigation', url: args[2] }, '*');
        return originalPushState.apply(history, args);
      };
      
      history.replaceState = function(...args) {
        console.log('ğŸ”€ replaceState:', args[2]);
        window.postMessage({ type: 'navigation', url: args[2] }, '*');
        return originalReplaceState.apply(history, args);
      };
    });
    
    // Listen for navigation messages
    await page.exposeFunction('onNavigation', (url: string) => {
      if (url && url.startsWith('/')) {
        discoveredUrls.add(new URL(url, baseUrl).href);
      }
    });
    
    await page.evaluate(() => {
      window.addEventListener('message', (e) => {
        if (e.data.type === 'navigation' && e.data.url) {
          (window as any).onNavigation(e.data.url);
        }
      });
    });
    
    // Method 3: Extract routes from JavaScript
    const jsRoutes = await page.evaluate(() => {
      const routes = new Set<string>();
      
      // Look for Next.js routes
      if ((window as any).__NEXT_DATA__) {
        const nextData = (window as any).__NEXT_DATA__;
        console.log('ğŸ”· Found Next.js data:', nextData);
        
        // Extract page paths
        if (nextData.page) routes.add(nextData.page);
        if (nextData.props?.pageProps?.href) routes.add(nextData.props.pageProps.href);
        
        // Look for route manifest
        if (nextData.runtimeConfig?.routes) {
          Object.values(nextData.runtimeConfig.routes).forEach((route: any) => {
            if (typeof route === 'string') routes.add(route);
          });
        }
      }
      
      // Look for React Router
      if ((window as any).__reactRouterVersion) {
        console.log('âš›ï¸ Found React Router');
        
        // Try to find route configuration
        const routerElements = document.querySelectorAll('[data-route], [data-path]');
        routerElements.forEach(el => {
          const route = el.getAttribute('data-route') || el.getAttribute('data-path');
          if (route) routes.add(route);
        });
      }
      
      // Look for Vue Router
      if ((window as any).$nuxt || (window as any).__VUE__) {
        console.log('ğŸŸ¢ Found Vue/Nuxt');
        
        // Extract routes from Vue Router
        try {
          const app = (window as any).__VUE__ || (window as any).$nuxt;
          if (app.$router && app.$router.options && app.$router.options.routes) {
            app.$router.options.routes.forEach((route: any) => {
              if (route.path) routes.add(route.path);
            });
          }
        } catch (e) {
          console.error('Error extracting Vue routes:', e);
        }
      }
      
      return Array.from(routes);
    });
    
    jsRoutes.forEach(route => {
      try {
        const fullUrl = new URL(route, baseUrl);
        discoveredUrls.add(fullUrl.href);
      } catch (e) {
        // Invalid URL
      }
    });
    
    // Method 4: Click on navigation elements
    const navSelectors = [
      'nav a',
      'nav button',
      '[role="navigation"] a',
      '[role="navigation"] button',
      '.nav-link',
      '.nav-item',
      'a[class*="nav"]',
      'button[class*="nav"]',
      '[data-testid*="nav"]',
      '[aria-label*="navigation"]'
    ];
    
    for (const selector of navSelectors) {
      const elements = await page.$$(selector);
      console.log(`ğŸ–±ï¸ Found ${elements.length} elements matching ${selector}`);
      
      for (const element of elements.slice(0, 5)) { // Limit to prevent too many clicks
        try {
          const isVisible = await element.isVisible();
          if (!isVisible) continue;
          
          const beforeUrl = page.url();
          
          // Hover first (might trigger dropdowns)
          await element.hover();
          await page.waitForTimeout(500);
          
          // Try to click
          await element.click({ timeout: 2000 });
          await page.waitForTimeout(1000);
          
          const afterUrl = page.url();
          if (afterUrl !== beforeUrl && afterUrl.startsWith(baseUrl)) {
            discoveredUrls.add(afterUrl);
            console.log(`âœ… Discovered via click: ${afterUrl}`);
            
            // Go back
            await page.goBack({ waitUntil: 'domcontentloaded' });
          }
        } catch (e) {
          // Click failed, continue
        }
      }
    }
    
    console.log(`ğŸ¯ Found ${discoveredUrls.size} potential SPA routes`);
    discoveredUrls.forEach((route, i) => console.log(`  ${i + 1}. ${route}`));
    
    return Array.from(discoveredUrls);
  }
}